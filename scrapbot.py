#!/usr/bin/env python
from bs4 import BeautifulSoup 
import sys
import re
import csv
from os import path
import json
from selenium import webdriver

SCRAP_URL = "http://botpoet.com/"
ToRemove = SCRAP_URL+"vote/"
BotMadeRE = "Generated by (.*) using (.*)"
firstRow = "title; isBot; madeBy; using; botPercentage; humanPercentage; text\n"

args = sys.argv

def parseAnswer(ans):
	isBot = False
	madeBy = None
	using = None
	search_res = re.search(BotMadeRE, ans, re.IGNORECASE)
	if search_res:
		isBot = True
		madeBy = search_res.group(1)
		using = search_res.group(2)
	else:
		madeBy = ans
	return isBot, madeBy, using


def extractResults(soup):
	bot = soup.find("div", {"class": "bot-progress-bar"}).get_text()[1:3]
	orNot = soup.find("div", {"class": "not-progress-bar"}).get_text()[1:3]
	return bot, orNot

def scraploop(titles, poems_file, browser):
	duplicates = 0
	writer = csv.writer(poems_file, delimiter=';')
	while(duplicates < 100):
		itsABot = browser.find_elements_by_name("submit")
		itsABot[0].click()
		currUrl = browser.current_url
		#Each poem use its title as a unique ID to get it by URL. THis is why I extract it from 
		poemTitle = currUrl[len(ToRemove):-1]
		if poemTitle in titlesArray:
			duplicates += 1
		else:
			soup = BeautifulSoup(browser.page_source, 'html.parser')
			result = soup.p.string #EITHER "Generated by ..." or the name of the human
			isBot, madeBy, using = parseAnswer(result)
			Bot, OrNot = extractResults(soup)
			poem = soup.find("pre", {"class": "poem"}).get_text()
			writer.writerow([
				poemTitle,
				isBot,
				madeBy,
				using,
				int(Bot),
				int(OrNot),
				poem
				])
			titles.append(poemTitle)

		nextLink = browser.find_element_by_link_text("Vote on another poem?")
		nextLink.click()


def main(title_array, poem_filename):
	#Initial connection to the website, and access  to the free play mode.
	browser = webdriver.Chrome()
	browser.get(SCRAP_URL)
	startLine = browser.find_element_by_link_text("Free play")
	startLine.click()

	with open(poem_filename, 'a') as f:
		scraploop(title_array, f, browser)
	browser.close()


if len(args) < 2:
	print("please specify filename where to store poems in first line command argument.")
else:
	prelude = args[1]
	titlesJson = prelude + "_titles.json"
	poemsFile = prelude + "_poems.csv"
	titlesArray = []
	if path.exists(titlesJson):
		with open(titlesJson, 'r') as f:
			if not path.exists(poemsFile):
				raise Exception("Titles exists for all poem but the actual poem data is lost ! Please locate the file of poem, or restart the scrapping.")
			titlesArray = json.load(f)['titles']
			
	if not path.exists(poemsFile):
		with open(poemsFile, 'w') as f:
			f.write(firstRow)

	main(titlesArray, poemsFile)
	with open(titlesJson, 'w') as f:
		#initializing empty file with correct structure.
		toDump = json.dumps(
			{"titles": titlesArray
			})
		f.write(toDump)
