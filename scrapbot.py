#!/usr/bin/env python
from bs4 import BeautifulSoup 
import sys
import re
from os import path
import json
from selenium import webdriver

SCRAP_URL = "http://botpoet.com/"
ToRemove = SCRAP_URL+"vote/"
BotMadeRE = "Generated by (.*) using (.*)"

args = sys.argv

def parseAnswer(ans):
	isBot = False
	madeBy = None
	using = None
	search_res = re.search(BotMadeRE, ans, re.IGNORECASE)
	if search_res:
		isBot = True
		madeBy = search_res.group(1)
		using = search_res.group(2)
	else:
		madeBy = ans
	return isBot, madeBy, using


def extractResults(soup):
	bot = soup.find("div", {"class": "bot-progress-bar"}).get_text()[1:3]
	orNot = soup.find("div", {"class": "not-progress-bar"}).get_text()[1:3]
	return bot, orNot

def main(title_array):

	#Initial connection to the website, and access  to the free play mode.
	browser = webdriver.Chrome()
	browser.get(SCRAP_URL)
	startLine = browser.find_element_by_link_text("Free play")
	startLine.click()

	###BEGIN OF THE SCRAP LOOP
	itsABot = browser.find_elements_by_name("submit")
	itsABot[0].click()
	currUrl = browser.current_url
	#Each poem use its title as a unique ID to get it by URL. THis is why I extract it from 
	poemTitle = currUrl[len(ToRemove):-1]
	soup = BeautifulSoup(browser.page_source, 'html.parser')
	result = soup.p.string #EITHER "Generated by ..." or the name of the human
	isBot, madeBy, using = parseAnswer(result)
	Bot, OrNot = extractResults(soup)
	poem = soup.find("pre", {"class": "poem"}).get_text()
	#print(poem)
	toDump = json.dumps({
			"title": poemTitle,
			"isBot": isBot,
			"madeBy": madeBy,
			"using" : using,
			"bot%": int(Bot),
			"human%": int(OrNot),
			"text": poem
		})
	print(toDump)
	nextLink = browser.find_element_by_link_text("Vote on another poem?")
	nextLink.click()
	#END OF SCRAP LOOP
	browser.close()


if len(args) < 2:
	print("please specify filename where to store poems in first line command argument.")
else:
	prelude = args[1]
	titlesJson = prelude + "_titles.json"
	poemsJson = prelude + "_poems.json"
	titlesArray = []
	if path.exists(titlesJson):
		with open(titlesJson, 'r') as f:
			if not path.exists(poemsJson):
				raise Exception("Titles exists for all poem but the actual poem data is lost ! Please locate the file of poem, or restart the scrapping.")
			titlesArray = json.load(f)['titles']
			
	poems = main(titlesArray)
	with open(titlesJson, 'w') as f:
		#initializing empty file with correct structure.
		toDump = json.dumps(
			{"titles": titlesArray
			})
		f.write(toDump)

	main(f)